{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Social Network Analysis: Six Degrees of Separation\n",
        "\n",
        "This notebook implements a comprehensive analysis of the soc-Pokec dataset to empirically test the \"six degrees of separation\" theory.\n",
        "\n",
        "## Project Overview\n",
        "- **Dataset**: soc-Pokec (Slovak social network) - 1.6M nodes, 30M edges\n",
        "- **Goal**: Compute average shortest path lengths, analyze network properties, and perform community detection\n",
        "- **Key Techniques**: Graph mining, centrality analysis, community detection, link prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Dataset URLs from Stanford SNAP\n",
        "URLS = {\n",
        "    'edges': 'https://snap.stanford.edu/data/soc-pokec-relationships.txt.gz',\n",
        "    'profiles': 'https://snap.stanford.edu/data/soc-pokec-profiles.txt.gz',\n",
        "    'readme': 'https://snap.stanford.edu/data/soc-pokec-readme.txt'\n",
        "}\n",
        "\n",
        "def download_file(url, dest_path):\n",
        "    \"\"\"Download a file with progress indication.\"\"\"\n",
        "    print(f\"Downloading {url}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    downloaded = 0\n",
        "    \n",
        "    with open(dest_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "            downloaded += len(chunk)\n",
        "            if total_size > 0:\n",
        "                pct = (downloaded / total_size) * 100\n",
        "                print(f\"\\rProgress: {pct:.1f}%\", end='', flush=True)\n",
        "    print(f\"\\nSaved to {dest_path}\")\n",
        "\n",
        "def extract_gzip(gz_path, output_path):\n",
        "    \"\"\"Extract a gzip file.\"\"\"\n",
        "    print(f\"Extracting {gz_path}...\")\n",
        "    with gzip.open(gz_path, 'rb') as f_in:\n",
        "        with open(output_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"Extracted to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and extract dataset files\n",
        "files_to_download = [\n",
        "    ('edges', DATA_DIR / 'soc-pokec-relationships.txt.gz', DATA_DIR / 'soc-pokec-relationships.txt'),\n",
        "    ('profiles', DATA_DIR / 'soc-pokec-profiles.txt.gz', DATA_DIR / 'soc-pokec-profiles.txt'),\n",
        "]\n",
        "\n",
        "for name, gz_path, txt_path in files_to_download:\n",
        "    if not txt_path.exists():\n",
        "        if not gz_path.exists():\n",
        "            download_file(URLS[name], gz_path)\n",
        "        extract_gzip(gz_path, txt_path)\n",
        "    else:\n",
        "        print(f\"{txt_path} already exists, skipping download.\")\n",
        "\n",
        "# Download readme\n",
        "readme_path = DATA_DIR / 'soc-pokec-readme.txt'\n",
        "if not readme_path.exists():\n",
        "    download_file(URLS['readme'], readme_path)\n",
        "else:\n",
        "    print(f\"{readme_path} already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify downloaded files\n",
        "for file in DATA_DIR.glob('*.txt'):\n",
        "    line_count = sum(1 for _ in open(file, 'r', encoding='utf-8', errors='ignore'))\n",
        "    size_mb = file.stat().st_size / (1024 * 1024)\n",
        "    print(f\"{file.name}: {line_count:,} lines, {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Return current memory usage in GB.\"\"\"\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / (1024 ** 3)\n",
        "\n",
        "print(f\"Initial memory usage: {get_memory_usage():.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load edges into a NetworkX graph\n",
        "print(\"Loading edge list...\")\n",
        "start_time = time.time()\n",
        "\n",
        "edges_file = DATA_DIR / 'soc-pokec-relationships.txt'\n",
        "G = nx.read_edgelist(str(edges_file), create_using=nx.DiGraph(), nodetype=int)\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"Loaded in {load_time:.2f} seconds\")\n",
        "print(f\"Directed Graph - Nodes: {G.number_of_nodes():,}, Edges: {G.number_of_edges():,}\")\n",
        "print(f\"Memory usage: {get_memory_usage():.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to undirected for separation calculations\n",
        "print(\"Converting to undirected graph...\")\n",
        "G_undirected = G.to_undirected()\n",
        "print(f\"Undirected Graph - Nodes: {G_undirected.number_of_nodes():,}, Edges: {G_undirected.number_of_edges():,}\")\n",
        "print(f\"Memory usage: {get_memory_usage():.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile columns based on soc-pokec-readme.txt\n",
        "# The dataset has 59 columns\n",
        "PROFILE_COLUMNS = [\n",
        "    'user_id', 'public', 'completion_percentage', 'gender', 'region', 'last_login',\n",
        "    'registration', 'AGE', 'body', 'I_am_working_in_field', 'spoken_languages',\n",
        "    'hobbies', 'I_most_enjoy_good_food', 'pets', 'body_type', 'my_eyesight',\n",
        "    'eye_color', 'hair_color', 'hair_type', 'completed_level_of_education',\n",
        "    'favourite_color', 'relation_to_smoking', 'relation_to_alcohol',\n",
        "    'sign_in_zodiac', 'on_pokec_i_teleasing_for', 'love_is_for_me',\n",
        "    'relation_to_casual_sex', 'my_partner_should_be', 'marital_status',\n",
        "    'children', 'relation_to_children', 'I_like_movies', 'I_like_watching_movie',\n",
        "    'I_like_music', 'I_mostly_like_listening_to_music', 'the_idea_of_good_evening',\n",
        "    'I_like_specialties_from_kitchen', 'fun', 'I_am_going_to_concerts',\n",
        "    'my_active_sports', 'my_passive_sports', 'profession', 'I_like_books',\n",
        "    'life_style', 'music', 'cars', 'politics', 'relationships', 'art_culture',\n",
        "    'hobbies_interests', 'science_technologies', 'computers_internet',\n",
        "    'education', 'sport', 'movies', 'travelling', 'health', 'companies_brands',\n",
        "    'more'\n",
        "]\n",
        "\n",
        "print(f\"Number of profile columns: {len(PROFILE_COLUMNS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load profiles with Pandas\n",
        "print(\"Loading profiles...\")\n",
        "start_time = time.time()\n",
        "\n",
        "profiles_file = DATA_DIR / 'soc-pokec-profiles.txt'\n",
        "profiles = pd.read_csv(\n",
        "    profiles_file,\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    names=PROFILE_COLUMNS,\n",
        "    dtype=str,\n",
        "    low_memory=False,\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"Loaded {len(profiles):,} profiles in {load_time:.2f} seconds\")\n",
        "print(f\"Memory usage: {get_memory_usage():.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview profiles\n",
        "profiles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess profiles\n",
        "profiles['user_id'] = pd.to_numeric(profiles['user_id'], errors='coerce')\n",
        "profiles = profiles.dropna(subset=['user_id'])\n",
        "profiles['user_id'] = profiles['user_id'].astype(int)\n",
        "profiles.set_index('user_id', inplace=True)\n",
        "\n",
        "# Convert age to numeric\n",
        "profiles['AGE'] = pd.to_numeric(profiles['AGE'], errors='coerce')\n",
        "\n",
        "# Fill missing values\n",
        "profiles.fillna('unknown', inplace=True)\n",
        "\n",
        "print(f\"Processed profiles: {len(profiles):,}\")\n",
        "print(f\"\\nAge statistics:\")\n",
        "print(profiles['AGE'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attach key attributes to graph nodes (sample for efficiency)\n",
        "print(\"Attaching profile attributes to graph nodes...\")\n",
        "attached_count = 0\n",
        "\n",
        "for node in G.nodes():\n",
        "    if node in profiles.index:\n",
        "        try:\n",
        "            G.nodes[node]['gender'] = profiles.loc[node, 'gender']\n",
        "            G.nodes[node]['age'] = profiles.loc[node, 'AGE']\n",
        "            G.nodes[node]['region'] = profiles.loc[node, 'region']\n",
        "            attached_count += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(f\"Attached attributes to {attached_count:,} nodes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Core Analysis - Compute Degrees of Separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Get the largest connected component for meaningful path calculations\n",
        "print(\"Finding connected components...\")\n",
        "components = list(nx.connected_components(G_undirected))\n",
        "print(f\"Number of connected components: {len(components)}\")\n",
        "\n",
        "# Get largest component\n",
        "largest_cc = max(components, key=len)\n",
        "print(f\"Largest component size: {len(largest_cc):,} nodes ({100*len(largest_cc)/G_undirected.number_of_nodes():.2f}% of graph)\")\n",
        "\n",
        "# Create subgraph of largest component\n",
        "G_lcc = G_undirected.subgraph(largest_cc).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample and compute shortest paths\n",
        "print(\"Computing degrees of separation (sampled)...\")\n",
        "\n",
        "NUM_SAMPLES = 10000  # Number of random pairs to sample\n",
        "nodes_list = list(G_lcc.nodes())\n",
        "path_lengths = []\n",
        "failed_paths = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(NUM_SAMPLES):\n",
        "    src, tgt = random.sample(nodes_list, 2)\n",
        "    try:\n",
        "        length = nx.shortest_path_length(G_lcc, src, tgt)\n",
        "        path_lengths.append(length)\n",
        "    except nx.NetworkXNoPath:\n",
        "        failed_paths += 1\n",
        "    \n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Progress: {i+1}/{NUM_SAMPLES} pairs computed\")\n",
        "\n",
        "compute_time = time.time() - start_time\n",
        "print(f\"\\nCompleted in {compute_time:.2f} seconds\")\n",
        "print(f\"Successful paths: {len(path_lengths):,}\")\n",
        "print(f\"Failed paths (disconnected): {failed_paths}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics\n",
        "if path_lengths:\n",
        "    avg_degree = np.mean(path_lengths)\n",
        "    median_degree = np.median(path_lengths)\n",
        "    std_degree = np.std(path_lengths)\n",
        "    min_degree = min(path_lengths)\n",
        "    max_degree = max(path_lengths)\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "    print(\"DEGREES OF SEPARATION ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Average degrees of separation: {avg_degree:.2f}\")\n",
        "    print(f\"Median degrees of separation: {median_degree:.2f}\")\n",
        "    print(f\"Standard deviation: {std_degree:.2f}\")\n",
        "    print(f\"Min path length: {min_degree}\")\n",
        "    print(f\"Max path length: {max_degree}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Distribution\n",
        "    print(\"\\nPath length distribution:\")\n",
        "    distribution = Counter(path_lengths)\n",
        "    for length in sorted(distribution.keys()):\n",
        "        count = distribution[length]\n",
        "        pct = 100 * count / len(path_lengths)\n",
        "        bar = 'â–ˆ' * int(pct / 2)\n",
        "        print(f\"  {length}: {count:,} ({pct:.1f}%) {bar}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute additional network metrics\n",
        "print(\"Computing network metrics...\")\n",
        "\n",
        "# Degree distribution\n",
        "degrees = [d for n, d in G_undirected.degree()]\n",
        "avg_degree_network = np.mean(degrees)\n",
        "print(f\"Average node degree: {avg_degree_network:.2f}\")\n",
        "\n",
        "# Density\n",
        "density = nx.density(G_undirected)\n",
        "print(f\"Network density: {density:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average clustering coefficient (sampled for large graphs)\n",
        "print(\"Computing clustering coefficient (sampled)...\")\n",
        "sample_nodes = random.sample(nodes_list, min(10000, len(nodes_list)))\n",
        "clustering_coeffs = nx.clustering(G_lcc, nodes=sample_nodes)\n",
        "avg_clustering = np.mean(list(clustering_coeffs.values()))\n",
        "print(f\"Average clustering coefficient (sampled): {avg_clustering:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reciprocity for directed graph\n",
        "reciprocity = nx.reciprocity(G)\n",
        "print(f\"Reciprocity (directed graph): {reciprocity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Advanced Mining Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Community Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import community as community_louvain\n",
        "\n",
        "# For large graphs, use a sample for community detection\n",
        "COMMUNITY_SAMPLE_SIZE = 50000\n",
        "\n",
        "print(f\"Sampling {COMMUNITY_SAMPLE_SIZE:,} nodes for community detection...\")\n",
        "sample_nodes_community = random.sample(nodes_list, min(COMMUNITY_SAMPLE_SIZE, len(nodes_list)))\n",
        "G_sample = G_lcc.subgraph(sample_nodes_community).copy()\n",
        "\n",
        "print(f\"Sample graph - Nodes: {G_sample.number_of_nodes():,}, Edges: {G_sample.number_of_edges():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Louvain community detection\n",
        "print(\"Running Louvain community detection...\")\n",
        "start_time = time.time()\n",
        "\n",
        "partition = community_louvain.best_partition(G_sample)\n",
        "\n",
        "detect_time = time.time() - start_time\n",
        "print(f\"Community detection completed in {detect_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze communities\n",
        "community_sizes = Counter(partition.values())\n",
        "num_communities = len(community_sizes)\n",
        "modularity = community_louvain.modularity(partition, G_sample)\n",
        "\n",
        "print(f\"\\nCommunity Detection Results:\")\n",
        "print(f\"Number of communities: {num_communities}\")\n",
        "print(f\"Modularity score: {modularity:.4f}\")\n",
        "print(f\"\\nTop 10 largest communities:\")\n",
        "for comm_id, size in community_sizes.most_common(10):\n",
        "    pct = 100 * size / len(partition)\n",
        "    print(f\"  Community {comm_id}: {size:,} nodes ({pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Centrality Measures (Identify Hubs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Degree centrality (fast)\n",
        "print(\"Computing degree centrality...\")\n",
        "degree_centrality = nx.degree_centrality(G_lcc)\n",
        "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"\\nTop 10 nodes by degree centrality:\")\n",
        "for node, centrality in top_degree:\n",
        "    degree = G_lcc.degree(node)\n",
        "    print(f\"  Node {node}: centrality={centrality:.6f}, degree={degree}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Betweenness centrality (sampled for efficiency)\n",
        "print(\"Computing betweenness centrality (sampled)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "betweenness_centrality = nx.betweenness_centrality(G_sample, k=500)  # Sample k nodes\n",
        "\n",
        "compute_time = time.time() - start_time\n",
        "print(f\"Computed in {compute_time:.2f} seconds\")\n",
        "\n",
        "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"\\nTop 10 nodes by betweenness centrality (sampled graph):\")\n",
        "for node, centrality in top_betweenness:\n",
        "    print(f\"  Node {node}: centrality={centrality:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PageRank (works well on large graphs)\n",
        "print(\"Computing PageRank...\")\n",
        "start_time = time.time()\n",
        "\n",
        "pagerank = nx.pagerank(G_lcc, alpha=0.85, max_iter=100)\n",
        "\n",
        "compute_time = time.time() - start_time\n",
        "print(f\"Computed in {compute_time:.2f} seconds\")\n",
        "\n",
        "top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"\\nTop 10 nodes by PageRank:\")\n",
        "for node, score in top_pagerank:\n",
        "    print(f\"  Node {node}: PageRank={score:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Link Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a smaller sample for link prediction\n",
        "LINK_PRED_SAMPLE = 5000\n",
        "sample_nodes_lp = random.sample(nodes_list, min(LINK_PRED_SAMPLE, len(nodes_list)))\n",
        "G_lp = G_lcc.subgraph(sample_nodes_lp).copy()\n",
        "\n",
        "print(f\"Link prediction sample - Nodes: {G_lp.number_of_nodes():,}, Edges: {G_lp.number_of_edges():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-edges for prediction\n",
        "print(\"Generating candidate pairs for link prediction...\")\n",
        "\n",
        "# Get existing edges\n",
        "existing_edges = set(G_lp.edges())\n",
        "\n",
        "# Sample non-edges\n",
        "non_edges = []\n",
        "lp_nodes = list(G_lp.nodes())\n",
        "attempts = 0\n",
        "max_attempts = 100000\n",
        "\n",
        "while len(non_edges) < 1000 and attempts < max_attempts:\n",
        "    u, v = random.sample(lp_nodes, 2)\n",
        "    if (u, v) not in existing_edges and (v, u) not in existing_edges:\n",
        "        non_edges.append((u, v))\n",
        "    attempts += 1\n",
        "\n",
        "print(f\"Generated {len(non_edges)} non-edge pairs for prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jaccard coefficient predictions\n",
        "print(\"Computing Jaccard coefficient predictions...\")\n",
        "jaccard_preds = list(nx.jaccard_coefficient(G_lp, non_edges))\n",
        "\n",
        "# Sort by score\n",
        "jaccard_preds.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 predicted links (Jaccard coefficient):\")\n",
        "for u, v, score in jaccard_preds[:10]:\n",
        "    print(f\"  ({u}, {v}): score={score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adamic-Adar predictions\n",
        "print(\"Computing Adamic-Adar predictions...\")\n",
        "aa_preds = list(nx.adamic_adar_index(G_lp, non_edges))\n",
        "\n",
        "# Sort by score\n",
        "aa_preds.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 predicted links (Adamic-Adar):\")\n",
        "for u, v, score in aa_preds[:10]:\n",
        "    print(f\"  ({u}, {v}): score={score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common neighbors predictions\n",
        "print(\"Computing Common Neighbors predictions...\")\n",
        "cn_preds = list(nx.common_neighbor_centrality(G_lp, non_edges))\n",
        "\n",
        "# Sort by score\n",
        "cn_preds.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nTop 10 predicted links (Common Neighbors):\")\n",
        "for u, v, score in cn_preds[:10]:\n",
        "    print(f\"  ({u}, {v}): score={score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Visualization and Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Create outputs directory\n",
        "OUTPUTS_DIR = Path('outputs')\n",
        "OUTPUTS_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Path Length Distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "sns.histplot(path_lengths, bins=range(min(path_lengths), max(path_lengths) + 2), \n",
        "             kde=True, ax=ax, color='steelblue', edgecolor='white')\n",
        "\n",
        "ax.axvline(avg_degree, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_degree:.2f}')\n",
        "ax.axvline(median_degree, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_degree:.2f}')\n",
        "ax.axvline(6, color='green', linestyle=':', linewidth=2, label='Six Degrees Theory')\n",
        "\n",
        "ax.set_xlabel('Degrees of Separation', fontsize=12)\n",
        "ax.set_ylabel('Frequency', fontsize=12)\n",
        "ax.set_title('Distribution of Degrees of Separation in soc-Pokec Network', fontsize=14)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUTS_DIR / 'path_lengths_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUTS_DIR / 'path_lengths_distribution.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 2: Degree Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Linear scale\n",
        "axes[0].hist(degrees, bins=100, color='steelblue', edgecolor='white', alpha=0.7)\n",
        "axes[0].set_xlabel('Degree', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Degree Distribution (Linear Scale)', fontsize=14)\n",
        "\n",
        "# Log-log scale (power law check)\n",
        "degree_counts = Counter(degrees)\n",
        "deg_values = list(degree_counts.keys())\n",
        "deg_freqs = list(degree_counts.values())\n",
        "\n",
        "axes[1].scatter(deg_values, deg_freqs, alpha=0.5, s=10, color='steelblue')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].set_xlabel('Degree (log)', fontsize=12)\n",
        "axes[1].set_ylabel('Frequency (log)', fontsize=12)\n",
        "axes[1].set_title('Degree Distribution (Log-Log Scale)', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUTS_DIR / 'degree_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUTS_DIR / 'degree_distribution.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 3: Community Size Distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "comm_sizes = list(community_sizes.values())\n",
        "sns.histplot(comm_sizes, bins=50, ax=ax, color='coral', edgecolor='white')\n",
        "\n",
        "ax.set_xlabel('Community Size', fontsize=12)\n",
        "ax.set_ylabel('Frequency', fontsize=12)\n",
        "ax.set_title(f'Community Size Distribution (Louvain, {num_communities} communities)', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUTS_DIR / 'community_size_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUTS_DIR / 'community_size_distribution.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 4: Clustering Coefficient Distribution\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "cc_values = list(clustering_coeffs.values())\n",
        "sns.histplot(cc_values, bins=50, ax=ax, color='mediumseagreen', edgecolor='white')\n",
        "\n",
        "ax.axvline(avg_clustering, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_clustering:.4f}')\n",
        "\n",
        "ax.set_xlabel('Clustering Coefficient', fontsize=12)\n",
        "ax.set_ylabel('Frequency', fontsize=12)\n",
        "ax.set_title('Distribution of Local Clustering Coefficients', fontsize=14)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUTS_DIR / 'clustering_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUTS_DIR / 'clustering_distribution.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5: Network Summary Statistics\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Create summary table\n",
        "summary_data = {\n",
        "    'Metric': [\n",
        "        'Total Nodes', 'Total Edges (Directed)', 'Total Edges (Undirected)',\n",
        "        'Largest Component Size', 'Average Degree', 'Network Density',\n",
        "        'Average Clustering Coeff.', 'Reciprocity (Directed)',\n",
        "        'Avg. Degrees of Separation', 'Number of Communities', 'Modularity'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f\"{G.number_of_nodes():,}\",\n",
        "        f\"{G.number_of_edges():,}\",\n",
        "        f\"{G_undirected.number_of_edges():,}\",\n",
        "        f\"{len(largest_cc):,}\",\n",
        "        f\"{avg_degree_network:.2f}\",\n",
        "        f\"{density:.6f}\",\n",
        "        f\"{avg_clustering:.4f}\",\n",
        "        f\"{reciprocity:.4f}\",\n",
        "        f\"{avg_degree:.2f}\",\n",
        "        f\"{num_communities}\",\n",
        "        f\"{modularity:.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "ax.axis('off')\n",
        "table = ax.table(\n",
        "    cellText=[[m, v] for m, v in zip(summary_data['Metric'], summary_data['Value'])],\n",
        "    colLabels=['Metric', 'Value'],\n",
        "    cellLoc='left',\n",
        "    loc='center',\n",
        "    colWidths=[0.6, 0.3]\n",
        ")\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(11)\n",
        "table.scale(1.2, 1.8)\n",
        "\n",
        "# Style header\n",
        "for i in range(2):\n",
        "    table[(0, i)].set_facecolor('#4472C4')\n",
        "    table[(0, i)].set_text_props(color='white', fontweight='bold')\n",
        "\n",
        "ax.set_title('soc-Pokec Network Summary Statistics', fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUTS_DIR / 'network_summary.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUTS_DIR / 'network_summary.png'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive visualization with Plotly (small sample)\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Sample a small subgraph for visualization\n",
        "VIZ_SAMPLE_SIZE = 1000\n",
        "viz_nodes = random.sample(nodes_list, min(VIZ_SAMPLE_SIZE, len(nodes_list)))\n",
        "G_viz = G_lcc.subgraph(viz_nodes).copy()\n",
        "\n",
        "print(f\"Visualization sample - Nodes: {G_viz.number_of_nodes()}, Edges: {G_viz.number_of_edges()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute layout\n",
        "print(\"Computing graph layout...\")\n",
        "pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)\n",
        "\n",
        "# Get community colors for viz nodes\n",
        "viz_partition = community_louvain.best_partition(G_viz)\n",
        "node_colors = [viz_partition.get(node, 0) for node in G_viz.nodes()]\n",
        "\n",
        "# Create edge traces\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "for edge in G_viz.edges():\n",
        "    x0, y0 = pos[edge[0]]\n",
        "    x1, y1 = pos[edge[1]]\n",
        "    edge_x.extend([x0, x1, None])\n",
        "    edge_y.extend([y0, y1, None])\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.3, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines'\n",
        ")\n",
        "\n",
        "# Create node traces\n",
        "node_x = [pos[node][0] for node in G_viz.nodes()]\n",
        "node_y = [pos[node][1] for node in G_viz.nodes()]\n",
        "node_degrees = [G_viz.degree(node) for node in G_viz.nodes()]\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        showscale=True,\n",
        "        colorscale='Viridis',\n",
        "        color=node_colors,\n",
        "        size=[min(5 + d/2, 20) for d in node_degrees],\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Community',\n",
        "            xanchor='left'\n",
        "        ),\n",
        "        line_width=0.5\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add hover text\n",
        "node_text = [f'Node: {node}<br>Degree: {G_viz.degree(node)}<br>Community: {viz_partition.get(node, \"N/A\")}' \n",
        "             for node in G_viz.nodes()]\n",
        "node_trace.text = node_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create figure\n",
        "fig = go.Figure(\n",
        "    data=[edge_trace, node_trace],\n",
        "    layout=go.Layout(\n",
        "        title=f'soc-Pokec Network Sample ({VIZ_SAMPLE_SIZE} nodes)',\n",
        "        titlefont_size=16,\n",
        "        showlegend=False,\n",
        "        hovermode='closest',\n",
        "        margin=dict(b=20, l=5, r=5, t=40),\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        plot_bgcolor='white'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.write_html(OUTPUTS_DIR / 'network_visualization.html')\n",
        "fig.show()\n",
        "print(f\"Saved: {OUTPUTS_DIR / 'network_visualization.html'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*70)\n",
        "print(\"SOCIAL NETWORK ANALYSIS: SIX DEGREES OF SEPARATION\")\n",
        "print(\"soc-Pokec Dataset Analysis Summary\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nðŸ“Š DATASET STATISTICS:\")\n",
        "print(f\"   â€¢ Total nodes: {G.number_of_nodes():,}\")\n",
        "print(f\"   â€¢ Total edges (directed): {G.number_of_edges():,}\")\n",
        "print(f\"   â€¢ Total edges (undirected): {G_undirected.number_of_edges():,}\")\n",
        "print(f\"   â€¢ Largest connected component: {len(largest_cc):,} nodes ({100*len(largest_cc)/G.number_of_nodes():.1f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ”— DEGREES OF SEPARATION:\")\n",
        "print(f\"   â€¢ Average: {avg_degree:.2f}\")\n",
        "print(f\"   â€¢ Median: {median_degree:.2f}\")\n",
        "print(f\"   â€¢ Range: {min_degree} - {max_degree}\")\n",
        "print(f\"   â€¢ Conclusion: {'Confirms' if avg_degree <= 6 else 'Exceeds'} six degrees theory!\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ NETWORK PROPERTIES:\")\n",
        "print(f\"   â€¢ Average node degree: {avg_degree_network:.2f}\")\n",
        "print(f\"   â€¢ Network density: {density:.6f}\")\n",
        "print(f\"   â€¢ Average clustering coefficient: {avg_clustering:.4f}\")\n",
        "print(f\"   â€¢ Reciprocity: {reciprocity:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ‘¥ COMMUNITY STRUCTURE:\")\n",
        "print(f\"   â€¢ Number of communities: {num_communities}\")\n",
        "print(f\"   â€¢ Modularity: {modularity:.4f}\")\n",
        "\n",
        "print(f\"\\nâ­ TOP INFLUENTIAL NODES (by PageRank):\")\n",
        "for i, (node, score) in enumerate(top_pagerank[:5], 1):\n",
        "    print(f\"   {i}. Node {node} (score: {score:.6f})\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ OUTPUTS SAVED:\")\n",
        "for f in OUTPUTS_DIR.glob('*'):\n",
        "    print(f\"   â€¢ {f.name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON for later use\n",
        "import json\n",
        "\n",
        "results = {\n",
        "    'dataset': {\n",
        "        'name': 'soc-Pokec',\n",
        "        'nodes': G.number_of_nodes(),\n",
        "        'edges_directed': G.number_of_edges(),\n",
        "        'edges_undirected': G_undirected.number_of_edges(),\n",
        "        'largest_component_size': len(largest_cc)\n",
        "    },\n",
        "    'degrees_of_separation': {\n",
        "        'samples': NUM_SAMPLES,\n",
        "        'average': avg_degree,\n",
        "        'median': median_degree,\n",
        "        'std': std_degree,\n",
        "        'min': min_degree,\n",
        "        'max': max_degree\n",
        "    },\n",
        "    'network_metrics': {\n",
        "        'average_degree': avg_degree_network,\n",
        "        'density': density,\n",
        "        'average_clustering': avg_clustering,\n",
        "        'reciprocity': reciprocity\n",
        "    },\n",
        "    'community_detection': {\n",
        "        'algorithm': 'Louvain',\n",
        "        'num_communities': num_communities,\n",
        "        'modularity': modularity\n",
        "    },\n",
        "    'top_nodes': {\n",
        "        'by_pagerank': [(int(n), float(s)) for n, s in top_pagerank[:10]],\n",
        "        'by_degree': [(int(n), float(s)) for n, s in top_degree[:10]]\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(OUTPUTS_DIR / 'analysis_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {OUTPUTS_DIR / 'analysis_results.json'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nâœ… Analysis complete!\")\n",
        "print(f\"Memory usage: {get_memory_usage():.2f} GB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
